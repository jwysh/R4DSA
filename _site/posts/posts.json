[
  {
    "path": "posts/welcome/",
    "title": "DataViz Assignment",
    "description": "VAST Mini Challenge 3",
    "author": [
      {
        "name": "Joyce WOON Shi Hui",
        "url": "https://www.linkedin.com/in/joycewoonsh/"
      }
    ],
    "date": "2021-07-25",
    "categories": [],
    "contents": "\r\n1 Case Introduction\r\nVast Challenge: Mini-Challenge 3 poses a social media and text analysis challenge.\r\nWe are to perform a retrospective analysis based on limited information about what took place in order to identify risks and how they could have been mitigated more effectively.\r\n1.1 Data Source\r\nThe single data stream available comes from two major sources:\r\nMicroblog records that have been identified by automated filters as being potentially relevant to the ongoing incident\r\nText transcripts of emergency dispatches by the Abila, Kronos local police and fire departments.\r\n1.2 Tasks and Questions\r\nUsing visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam?\r\nUse visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected.\r\nIf you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively?\r\n2 Code\r\n2.1 Install and Load R Packages\r\nThis is so that the code can run.\r\n\r\n\r\npackages = c('tm', 'SnowballC', 'wordcloud', 'RColorBrewer', 'dplyr',\r\n             'Rcpp', 'lubridate', 'stringr', 'tidytext', 'igraph',\r\n             'ggraph', 'ggplot2', 'rtweet', 'raster', 'sf',\r\n             'tmap', 'tidyr', 'tidyverse','tibble')\r\n\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n    install.packages(p)\r\n  }\r\n  library(p, character.only = T)\r\n}\r\n\r\n\r\n\r\n2.2 Setting up Code\r\n2.2.1 Set seed for reproducibility\r\n\r\n\r\nset.seed(1234) \r\n\r\n\r\n\r\n2.2.2 Import csv files\r\n\r\n\r\ndf1 <- read_csv(\"csv-1700-1830.csv\")\r\ndf2 <- read_csv(\"csv-1831-2000.csv\")\r\ndf3 <- read_csv(\"csv-2001-2131.csv\")\r\n\r\ncolnames(df1)[2] <- c(\"dt\")\r\ncolnames(df2)[2] <- c(\"dt\")\r\ncolnames(df3)[2] <- c(\"dt\")\r\n\r\n\r\n\r\n2.2.3 Change message to lowercase\r\n\r\n\r\ndf1$message <- tolower(df1$message)\r\ndf2$message <- tolower(df2$message)\r\ndf3$message <- tolower(df3$message)\r\n\r\n\r\n\r\n2.2.3 Change datetime\r\n\r\n\r\ndf1$dt = df1$dt - 20140123000000\r\ndf2$dt = df2$dt - 20140123000000\r\ndf3$dt = df3$dt - 20140123000000\r\n\r\ndf1$dt <- format(strptime(df1$dt, format=\"%H%M%S\"), format = \"%H:%M:%S\")\r\ndf2$dt <- format(strptime(df2$dt, format=\"%H%M%S\"), format = \"%H:%M:%S\")\r\ndf3$dt <- format(strptime(df3$dt, format=\"%H%M%S\"), format = \"%H:%M:%S\")\r\n\r\n\r\n\r\n2.2.4 Find out range of dates per dataframe\r\ndf1 is from 17:00:00 to 18:34:00\r\n\r\n\r\nmin(df1$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"17:00:00\"\r\n\r\nmax(df1$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"18:34:00\"\r\n\r\ndf2 is from 18:31:00 to 20:04:00\r\n\r\n\r\nmin(df2$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"18:31:00\"\r\n\r\nmax(df2$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"20:04:00\"\r\n\r\ndf3 is from 20:01:00 to 21:34:45\r\n\r\n\r\nmin(df3$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"20:01:00\"\r\n\r\nmax(df3$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"21:34:45\"\r\n\r\nIt looks like there is some overlap in time in df1, df2, df3.\r\n2.2.5 Create new dataframes\r\ndf1 17:00 to <18:30\r\ndf2 18:30 to <20:00\r\ndf3 >=20:00\r\ndf4 17:00:00 to 21:34:45\r\n\r\n\r\n# Bind df1, df2 and df3 together to create df4\r\ndf4 <- rbind(df1, df2, df3)\r\n\r\n# df1: Extract 17:00 to <18:30\r\ndf1 <- df4[df4$dt < \"18:30:00\",]\r\ndf1$period <- \"17:00 to <18:30\"\r\n\r\n# df2: Extract 18:30 to <20:00\r\ndf2 <- df4[df4$dt >= \"18:30:00\" & df4$dt < \"20:00:00\",]\r\ndf2$period <- \"18:30 to <20:00\"\r\n\r\n# df3: Extract >=20:00\r\ndf3 <- df4[df4$dt >= \"20:00:00\",]\r\ndf3$period <- \">=20:00\"\r\n\r\n\r\n\r\n2.2.6 Find out range of dates per new dataframe\r\nnew df1 is from 17:00:00 to 18:29:44\r\n\r\n\r\nmin(df1$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"17:00:00\"\r\n\r\nmax(df1$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"18:29:44\"\r\n\r\nnew df2 is from 18:30:00 to 19:59:52\r\n\r\n\r\nmin(df2$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"18:30:00\"\r\n\r\nmax(df2$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"19:59:52\"\r\n\r\nnew df3 is from 20:00:13 to 21:34:45\r\n\r\n\r\nmin(df3$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"20:00:13\"\r\n\r\nmax(df3$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"21:34:45\"\r\n\r\nnew df4 is from 17:00:00 to 21:34:45\r\n\r\n\r\nmin(df4$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"17:00:00\"\r\n\r\nmax(df4$dt,na.rm = TRUE)\r\n\r\n\r\n[1] \"21:34:45\"\r\n\r\nThere are no overlaps now!\r\n2.2.7 Investigating if there is any spam\r\nWe use wordclouds to see the most frequent words.\r\nWordcloud for df1\r\n\r\n\r\ntext <- df1$message\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc1 <- data.frame(word = names(words),freq=words)\r\n\r\n\r\nwordcloud(words = wc1$word, freq = wc1$freq, min.freq = 1,\r\n          max.words=175, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nWordcloud for df2\r\n\r\n\r\ntext <- df2$message\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc2 <- data.frame(word = names(words),freq=words)\r\n\r\n\r\nwordcloud(words = wc2$word, freq = wc2$freq, min.freq = 1,\r\n          max.words=175, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nWordcloud for df3\r\n\r\n\r\ntext <- df3$message\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc3 <- data.frame(word = names(words),freq=words)\r\n\r\n\r\nwordcloud(words = wc3$word, freq = wc3$freq, min.freq = 1,\r\n          max.words=175, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nWordcloud for df4\r\n\r\n\r\ntext <- df4$message\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc4 <- data.frame(word = names(words),freq=words)\r\n\r\n\r\nwordcloud(words = wc4$word, freq = wc4$freq, min.freq = 1,\r\n          max.words=175, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nBarplot for df4\r\n\r\n\r\nbarplot(wc4[1:10,]$freq, las = 2, names.arg = wc4[1:10,]$word,\r\n        col =\"lightblue\", main =\"Most frequent words\",\r\n        ylab = \"Word frequencies\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nwc4[1:10,]\r\n\r\n\r\n                 word freq\r\npokrally     pokrally 1376\r\nkronosstar kronosstar  937\r\nabila           abila  549\r\npok               pok  489\r\nfire             fire  401\r\nabilapost   abilapost  400\r\npolice         police  312\r\nrally           rally  300\r\ncan               can  224\r\nlife             life  210\r\n\r\nAs we can see, pokrally and kronosstar seems to have the most frequency and that they are hashtags. To investigate.\r\nInvestigate pokrally.\r\n\r\n\r\n# Filtering out pokrally hashtag (ht1_)\r\nht1_df4 <- df4 %>%\r\n  filter(str_detect(message, '#pokrally'))\r\n\r\n\r\n\r\n\r\n\r\nht1_df4\r\n\r\n\r\n# A tibble: 1,399 x 7\r\n   type   dt    author   message           latitude longitude location\r\n   <chr>  <chr> <chr>    <chr>                <dbl>     <dbl> <chr>   \r\n 1 mbdata 17:0~ ourcoun~ pok rally in the~       NA        NA <NA>    \r\n 2 mbdata 17:0~ KronosQ~ success is walki~       NA        NA <NA>    \r\n 3 mbdata 17:0~ KronosQ~ a person who nev~       NA        NA <NA>    \r\n 4 mbdata 17:0~ KronosQ~ the only person ~       NA        NA <NA>    \r\n 5 mbdata 17:0~ KronosQ~ knowledge is bei~       NA        NA <NA>    \r\n 6 mbdata 17:0~ KronosQ~ the meaning of l~       NA        NA <NA>    \r\n 7 mbdata 17:0~ KronosQ~ if you're going ~       NA        NA <NA>    \r\n 8 mbdata 17:0~ KronosQ~ you miss 100% of~       NA        NA <NA>    \r\n 9 mbdata 17:0~ KronosQ~ you may only suc~       NA        NA <NA>    \r\n10 mbdata 17:0~ KronosQ~ i didnt fail the~       NA        NA <NA>    \r\n# ... with 1,389 more rows\r\n\r\nWe note that for the pokrally hashtag, there is a user “KronosQuoth” whose content seems to be spam.\r\nRemove KronosQuoth.\r\n\r\n\r\n# Remove KronosQuoth\r\ndf1 <- df1 %>%\r\n  filter(!str_detect(author, 'KronosQuoth'))\r\ndf2 <- df2 %>%\r\n  filter(!str_detect(author, 'KronosQuoth'))\r\ndf3 <- df3%>%\r\n  filter(!str_detect(author, 'KronosQuoth'))\r\ndf4 <- df4%>%\r\n  filter(!str_detect(author, 'KronosQuoth'))\r\n\r\n\r\n\r\n\r\n\r\n# Check pokrally hashtag after removing KronosQuoth\r\nht1_df4 <- df4 %>%\r\n  filter(str_detect(message, '#pokrally'))\r\nht1_df4\r\n\r\n\r\n# A tibble: 134 x 7\r\n   type   dt    author   message           latitude longitude location\r\n   <chr>  <chr> <chr>    <chr>                <dbl>     <dbl> <chr>   \r\n 1 mbdata 17:0~ ourcoun~ \"pok rally in th~     NA        NA   <NA>    \r\n 2 mbdata 17:0~ choconi~ \"marek - terrori~     NA        NA   <NA>    \r\n 3 mbdata 17:0~ wiseWor~ \"rt @ourcountryo~     NA        NA   <NA>    \r\n 4 mbdata 17:1~ POK      \"show your suppo~     NA        NA   <NA>    \r\n 5 mbdata 17:1~ choconi~ \"lovely words - ~     NA        NA   <NA>    \r\n 6 mbdata 17:1~ anarege~ \"sylvia marek is~     NA        NA   <NA>    \r\n 7 mbdata 17:1~ rnbwBri~ \"rt @choconibbs ~     NA        NA   <NA>    \r\n 8 mbdata 17:1~ sarajane \"rt @pok show yo~     NA        NA   <NA>    \r\n 9 mbdata 17:1~ truthfo~ \"cant wait to he~     36.1      24.9 <NA>    \r\n10 mbdata 17:1~ choconi~ \"blather-blather~     NA        NA   <NA>    \r\n# ... with 124 more rows\r\n\r\nThere is less spam now. However, to note there are RTs, which should be removed.\r\nRemove RTs.\r\n\r\n\r\ndf1 <- df1 %>%\r\n  filter(!str_detect(message, 'rt @'))\r\ndf2 <- df2 %>%\r\n  filter(!str_detect(message, 'rt @'))\r\ndf3 <- df3%>%\r\n  filter(!str_detect(message, 'rt @'))\r\ndf4 <- df4%>%\r\n  filter(!str_detect(message, 'rt @'))\r\n\r\n\r\n\r\n\r\n\r\n# Check pokrally hashtag after removing RTs\r\nht1_df4 <- df4 %>%\r\n  filter(str_detect(message, '#pokrally'))\r\nht1_df4\r\n\r\n\r\n# A tibble: 86 x 7\r\n   type   dt    author   message           latitude longitude location\r\n   <chr>  <chr> <chr>    <chr>                <dbl>     <dbl> <chr>   \r\n 1 mbdata 17:0~ ourcoun~ \"pok rally in th~     NA        NA   <NA>    \r\n 2 mbdata 17:0~ choconi~ \"marek - terrori~     NA        NA   <NA>    \r\n 3 mbdata 17:1~ POK      \"show your suppo~     NA        NA   <NA>    \r\n 4 mbdata 17:1~ choconi~ \"lovely words - ~     NA        NA   <NA>    \r\n 5 mbdata 17:1~ anarege~ \"sylvia marek is~     NA        NA   <NA>    \r\n 6 mbdata 17:1~ truthfo~ \"cant wait to he~     36.1      24.9 <NA>    \r\n 7 mbdata 17:1~ choconi~ \"blather-blather~     NA        NA   <NA>    \r\n 8 mbdata 17:1~ truccot~ \"sylvia rocks! y~     36.1      24.9 <NA>    \r\n 9 mbdata 17:1~ choconi~ \"\\\"look to crime~     NA        NA   <NA>    \r\n10 mbdata 17:1~ truthfo~ \"lots of cops ho~     36.1      24.9 <NA>    \r\n# ... with 76 more rows\r\n\r\nThere is less spam now.\r\nInvestigate kronosstar.\r\n\r\n\r\n# Filtering out kronosstar hashtag (ht2_)\r\nht2_df4 <- df4 %>%\r\n  filter(str_detect(message, '#kronosstar'))\r\nht2_df4\r\n\r\n\r\n# A tibble: 56 x 7\r\n   type   dt     author  message           latitude longitude location\r\n   <chr>  <chr>  <chr>   <chr>                <dbl>     <dbl> <chr>   \r\n 1 mbdata 17:00~ Kronos~ \"pok rally to st~       NA        NA <NA>    \r\n 2 mbdata 17:10~ Kronos~ \"#apd has closed~       NA        NA <NA>    \r\n 3 mbdata 17:30~ Kronos~ \"#apd has closed~       NA        NA <NA>    \r\n 4 mbdata 17:40~ junkma~ \"he who hesitate~       NA        NA <NA>    \r\n 5 mbdata 17:44~ meds4r~ \"he who hesitate~       NA        NA <NA>    \r\n 6 mbdata 17:54~ masked~ \"he who hesitate~       NA        NA <NA>    \r\n 7 mbdata 18:42~ Kronos~ \"abila fire depa~       NA        NA <NA>    \r\n 8 mbdata 18:44~ Kronos~ \"abila police de~       NA        NA <NA>    \r\n 9 mbdata 18:45~ Kronos~ \"smoke is coming~       NA        NA <NA>    \r\n10 mbdata 18:46~ Kronos~ \"police have arr~       NA        NA <NA>    \r\n# ... with 46 more rows\r\n\r\nIt seems that kronosstar is not spam.\r\n2.3 Create Word Clouds\r\nTo note that abila, pok and abilapost are also high frequency words that do not add to the analysis. Thus, they will be removed.\r\nWordcloud for df1\r\n\r\n\r\ntext <- df1$message\r\ntext <- str_replace_all(text,\"@[a-z,A-Z]*\",\"\") # Remove @\r\ntext <- str_replace_all(text,\"abila\",\"\") # Remove abila\r\ntext <- str_replace_all(text,\"pok\",\"\") # Remove pok\r\ntext <- str_replace_all(text,\"abilapost\",\"\") # Remove abilapost\r\ntext <- str_replace_all(text,\"kronosstar\",\"\") # Remove kronosstar\r\n\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\n\r\n\r\n\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc1 <- data.frame(word = names(words),freq=words)\r\n\r\nwordcloud(words = wc1$word, freq = wc1$freq, min.freq = 1,\r\n          max.words=200, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nBarplot for df1\r\n\r\n\r\nbarplot(wc1[1:10,]$freq, las = 2, names.arg = wc1[1:10,]$word,\r\n        col =\"lightblue\", main =\"Most frequent words\",\r\n        ylab = \"Word frequencies\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nwc1[1:10,]\r\n\r\n\r\n           word freq\r\nrally     rally  140\r\ngrammar grammar   49\r\nviktore viktore   34\r\npeople   people   25\r\njakab     jakab   23\r\nlucio     lucio   23\r\npark       park   22\r\nnewman   newman   22\r\nsylvia   sylvia   20\r\nstefano stefano   19\r\n\r\nWordcloud for df2\r\n\r\n\r\ntext <- df2$message\r\ntext <- str_replace_all(text,\"@[a-z,A-Z]*\",\"\") # Remove @\r\ntext <- str_replace_all(text,\"abila\",\"\") # Remove abila\r\ntext <- str_replace_all(text,\"pok\",\"\") # Remove pok\r\ntext <- str_replace_all(text,\"abilapost\",\"\") # Remove abilapost\r\ntext <- str_replace_all(text,\"kronosstar\",\"\") # Remove kronosstar\r\n\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ninspect(docs)\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc2 <- data.frame(word = names(words),freq=words)\r\n\r\nwordcloud(words = wc2$word, freq = wc2$freq, min.freq = 1,\r\n          max.words=200, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nBarplot for df2\r\n\r\n\r\nbarplot(wc2[1:10,]$freq, las = 2, names.arg = wc2[1:10,]$word,\r\n        col =\"lightblue\", main =\"Most frequent words\",\r\n        ylab = \"Word frequencies\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nwc2[1:10,]\r\n\r\n\r\n           word freq\r\nfire       fire  113\r\nrally     rally   94\r\ngrammar grammar   59\r\npolice   police   59\r\njust       just   51\r\ndancing dancing   37\r\ndolphin dolphin   37\r\napd         apd   37\r\ngoing     going   36\r\npost       post   35\r\n\r\nWordcloud for df3\r\n\r\n\r\ntext <- df3$message\r\ntext <- str_replace_all(text,\"@[a-z,A-Z]*\",\"\") # Remove @\r\ntext <- str_replace_all(text,\"abila\",\"\") # Remove abila\r\ntext <- str_replace_all(text,\"pok\",\"\") # Remove pok\r\ntext <- str_replace_all(text,\"abilapost\",\"\") # Remove abilapost\r\ntext <- str_replace_all(text,\"kronosstar\",\"\") # Remove kronosstar\r\n\r\ndocs <- Corpus(VectorSource(text))\r\ndocs <- docs %>%\r\n  tm_map(removeNumbers) %>%\r\n  tm_map(removePunctuation) %>%\r\n  tm_map(stripWhitespace)\r\ndocs <- tm_map(docs, content_transformer(tolower))\r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ninspect(docs)\r\ndtm <- TermDocumentMatrix(docs) \r\nmatrix <- as.matrix(dtm) \r\nwords <- sort(rowSums(matrix),decreasing=TRUE) \r\nwc3 <- data.frame(word = names(words),freq=words)\r\n\r\nwordcloud(words = wc3$word, freq = wc3$freq, min.freq = 1,\r\n          max.words=200, random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\nBarplot for df3\r\n\r\n\r\nbarplot(wc3[1:10,]$freq, las = 2, names.arg = wc3[1:10,]$word,\r\n        col =\"lightblue\", main =\"Most frequent words\",\r\n        ylab = \"Word frequencies\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nwc3[1:10,]\r\n\r\n\r\n             word freq\r\ntag           tag   63\r\nrally       rally   59\r\ngrammar   grammar   49\r\nstandoff standoff   47\r\nvan           van   42\r\nfire         fire   40\r\napd           apd   30\r\npolice     police   28\r\nhostages hostages   26\r\npost         post   26\r\n\r\n3 Observations\r\nThe questions posed in Section 1 will be answered with observations from Section 2.\r\n1. Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam?\r\nAccording to 2.2.7, there seems to be normal messages, RTs and spam by authors such as KronosQuoth. RTs and spam are not meaningful reports due to RTs being the repeating of messages and spam not adding to the analysis. It is very common for social media such as Twitter to have bots that post quotes queued in historically. KronosQuoth looks to be one such bot and thus was removed.\r\nOther words that were removed due to high frequency and being not useful to the analysis would be:\r\nabila\r\npok\r\nabilapost\r\nkronosstar\r\nAbila and Abilapost are a place and an author respectively. POK and Kronosstar are just hashtags.\r\n2. Use visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected.\r\nObservations from the above 3 wordclouds:\r\ndf1 (17:00 to <18:30)\r\nThere was a rally going on, police were involved but nothing relatively dangerous was happening.\r\n“Rally” was the highest frequency word at 140. The rest of the highest frequency words are 49 and below. As such, the most notable thing that happened in df1 was a rally.\r\ndf2 (18:30 to <20:00)\r\nFire seems to be highest frequency. Other risky words start appearing such as shooting, evacuation. Places to take note of would be Dancing Dolphin Apartment Complex and Gelato Galore.\r\n“Fire” is the highest frequency word at 113. Notable words in top 10 words include “police”, “dancing” and “dolphin”.\r\ndf3 (20:00 to <21:35)\r\nThere are many more risk words here other than “fire” such as “terrorists”, “hostage”, etc. However, it looks like there have been response to assist the situation with “standoff”, “firefighter”, “swat”, etc. “Dancing” and “Dolphin” seems to be appearing again, which might mean that the danger is occuring there.\r\n“Tag” is the highest frequency word at 63. Notable words in top 10 words include “fire” at 40, “apd”, “police”. It seems like a response happened in df3 and fire was handled.\r\nOverall observation for Qn 2:\r\nLevel of risk to public according to the observations above are df2 > df3 > df1 in the sense that df1 did not have the risk occuring, df2 had the risk occuring and df3 with the risk occuring but at least it was handled by relevant forces like the police, firemen, etc. df2 was the time period where people were most in trouble.\r\nWe can also see this from the number of times the word fire was used between df2 and df3 with 309 vs 103 respectively. It seems like the number of people in danger decreased between df2 and df3.\r\n3. If you were able to send a team of first responders to any single place, where would it be? Provide your rationale. How might your response be different if you had to respond to the events in real time rather than retrospectively?\r\nAccording to the word clouds, Dancing Dolphin seems to be mentioned the most in df2. I would send first responders to that place first during df2.\r\nIf I had to respond to events in real time, I would choose the following:\r\ndf1: Park\r\ndf2: Dancing Dolphin\r\ndf3: Dancing Dolphin\r\nBut realistically, no one will be sent during df1 because nothing has occured and the police are somewhat covering the rally. However, if something did happen during that timeframe, I would send first responders there.\r\n\r\n\r\n\r\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2021-07-25T04:30:58+08:00",
    "input_file": "welcome.knit.md"
  }
]
